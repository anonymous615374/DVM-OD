{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from Model.DVM_OD import DVM_OD\n",
    "\n",
    "from sklearn.metrics import (roc_auc_score, precision_score, average_precision_score, recall_score,f1_score, accuracy_score,matthews_corrcoef)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, RobustScaler, MinMaxScaler, MaxAbsScaler, Normalizer\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_vector(point_X, point_Y):\n",
    "    \"\"\"\n",
    "    Calculate pairwise Euclidean distance between two sets of points.\n",
    "    \n",
    "    Args:\n",
    "        point_X (ndarray): Array of shape (N_train, d) where N_train is the number of training samples and d is the number of features.\n",
    "        point_Y (ndarray): Array of shape (N_test, d) where N_test is the number of test samples and d is the number of features.\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: Distance matrix of shape (N_test, N_train) containing Euclidean distances between each pair of points.\n",
    "    \"\"\"\n",
    "    # Compute squared norms for each point\n",
    "    norm_X = np.sum(point_X**2, axis=1)  # (N_train,)\n",
    "    norm_Y = np.sum(point_Y**2, axis=1)  # (N_test,)\n",
    "    \n",
    "    # Compute the dot product between the two sets of points\n",
    "    dot_product = np.dot(point_Y, point_X.T)  # (N_test, N_train)\n",
    "    \n",
    "    # Apply Euclidean distance formula\n",
    "    distance = np.sqrt(abs(norm_Y[:, np.newaxis] + norm_X[np.newaxis, :] - 2 * dot_product))\n",
    "    return distance\n",
    "\n",
    "def preprocess_data_OC(train_data, test_data):\n",
    "    \"\"\"\n",
    "    Preprocess the training and testing data by separating features and labels,\n",
    "    and return the preprocessed feature sets and labels.\n",
    "    \n",
    "    Args:\n",
    "        train_data (DataFrame): Training data containing features and labels.\n",
    "        test_data (DataFrame): Testing data containing features and labels.\n",
    "        \n",
    "    Returns:\n",
    "        X_train (ndarray): Preprocessed training features.\n",
    "        y_train (ndarray): Preprocessed training labels.\n",
    "        X_test (ndarray): Preprocessed testing features.\n",
    "        y_test (ndarray): Preprocessed testing labels.\n",
    "    \"\"\"\n",
    "    print(\"..............................Data Overview................................\")\n",
    "    print(\"Train Data Shape:\", train_data.shape)\n",
    "    print(\"Test Data Shape:\", test_data.shape)\n",
    "\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    X_train_total = train_data.iloc[:, :-1].to_numpy()\n",
    "    y_train_total = train_data.iloc[:, -1].to_numpy()\n",
    "\n",
    "    X_train = X_train_total[y_train_total == 0]\n",
    "    y_train = y_train_total[y_train_total == 0]\n",
    "\n",
    "    print(\"Train Data Labels [0]:\", np.unique(y_train))\n",
    "\n",
    "    # Prepare test data\n",
    "    X_test = test_data.iloc[:, :-1].to_numpy()\n",
    "    y_test = test_data.iloc[:, -1].to_numpy()\n",
    "\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_features = X_train.shape[1]\n",
    "    print(\"Number of samples:\", n_samples)\n",
    "    print(\"Number of features:\", n_features)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def Model_evaluating(y_true, y_predict, y_scores):\n",
    "    \"\"\"\n",
    "    Evaluate the model using various metrics such as AUC, AUCPR, Accuracy, MCC, F1-score, Precision, and Recall.\n",
    "    \n",
    "    Args:\n",
    "        y_true (ndarray): True labels.\n",
    "        y_predict (ndarray): Predicted labels.\n",
    "        y_scores (ndarray): Predicted probabilities for each class.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of evaluation metrics.\n",
    "    \"\"\"\n",
    "    print(\"..............................Report Parameter...............................\")\n",
    "    \n",
    "    # Calculate MCC (Matthews Correlation Coefficient)\n",
    "    mcc = matthews_corrcoef(y_true, y_predict)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_true, y_predict)\n",
    "    \n",
    "    # Calculate PPV (Positive Predictive Value) or Precision\n",
    "    ppv = precision_score(y_true, y_predict)\n",
    "    \n",
    "    # Calculate TPR (True Positive Rate) or Recall/Sensitivity\n",
    "    tpr = recall_score(y_true, y_predict)\n",
    "    \n",
    "    # Calculate Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_predict)\n",
    "    \n",
    "    # Calculate AUC (Area Under ROC Curve)\n",
    "    AUC = roc_auc_score(y_true, y_scores[:, 1])\n",
    "    \n",
    "    # Calculate AUCPR (Area Under Precision-Recall Curve)\n",
    "    AUCPR = average_precision_score(y_true, y_scores[:, 1])\n",
    "    \n",
    "    # Print out the evaluation metrics\n",
    "    print(\"AUCROC:\", AUC * 100)\n",
    "    print(\"AUCPR:\", AUCPR * 100)\n",
    "    print(\"Accuracy:\", accuracy * 100)\n",
    "    print(\"MCC:\", mcc)\n",
    "    print(\"F1 score:\", f1)\n",
    "    print(\"PPV (Precision):\", ppv)\n",
    "    print(\"TPR (Recall):\", tpr)\n",
    "    return [AUC * 100, AUCPR * 100, accuracy * 100, mcc, f1, ppv, tpr]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define three arrays of dataset names corresponding to different domains (Classical, NLP, CV)\n",
    "## You can uncomment the datasets you want to run\n",
    "dataset_name1 = np.array([  # Classical Datasets\n",
    "    '1_ALOI'])\n",
    "    # , '2_annthyroid', '3_backdoor', '4_breastw', '5_campaign',\n",
    "    # '6_cardio', '7_Cardiotocography', '8_celeba', '9_census', '10_cover', '11_donors', '12_fault', '13_fraud', '14_glass',\n",
    "    # '15_Hepatitis', '16_http', '17_InternetAds', '18_Ionosphere', '19_landsat', '20_letter',\n",
    "    # '21_Lymphography', '22_magic.gamma', '23_mammography',\n",
    "    # '24_mnist', '25_musk', '26_optdigits', '27_PageBlocks',\n",
    "    # '28_pendigits', '29_Pima', '30_satellite',\n",
    "    # '31_satimage-2', '32_shuttle','33_skin', '34_smtp', '35_SpamBase',\n",
    "    # '36_speech', '37_Stamps', '38_thyroid', '39_vertebral',\n",
    "    # '40_vowels', '41_Waveform', '42_WBC', '43_WDBC', '44_Wilt',\n",
    "    # '45_wine', '46_WPBC', '47_yeast'])\n",
    "\n",
    "dataset_name2 = np.array([  # NLP Datasets\n",
    "    '20news_0'])\n",
    "    # ,'20news_1','20news_2','20news_3','20news_4','20news_5',\n",
    "    # 'agnews_0','agnews_1','agnews_2','agnews_3','amazon', 'imdb','yelp'])\n",
    "\n",
    "dataset_name3 = np.array([  # Computer Vision Datasets\n",
    "    'CIFAR10_0'])\n",
    "    # 'CIFAR10_1','CIFAR10_2','CIFAR10_3','CIFAR10_4','CIFAR10_5','CIFAR10_6','CIFAR10_7',\n",
    "    # 'CIFAR10_8','CIFAR10_9','FashionMNIST_0','FashionMNIST_1','FashionMNIST_2','FashionMNIST_3','FashionMNIST_4','FashionMNIST_5',\n",
    "    # 'FashionMNIST_6','FashionMNIST_7','FashionMNIST_8','FashionMNIST_9','SVHN_0','SVHN_1','SVHN_2','SVHN_3','SVHN_4',\n",
    "    # 'SVHN_5','SVHN_6','SVHN_7','SVHN_8','SVHN_9','MNIST-C_brightness', 'MNIST-C_canny_edges','MNIST-C_dotted_line', 'MNIST-C_fog',\n",
    "    # 'MNIST-C_glass_blur','MNIST-C_identity','MNIST-C_impulse_noise','MNIST-C_motion_blur','MNIST-C_rotate','MNIST-C_scale','MNIST-C_shear',\n",
    "    # 'MNIST-C_shot_noise','MNIST-C_spatter','MNIST-C_stripe','MNIST-C_translate','MNIST-C_zigzag','MVTec-AD_bottle',\n",
    "    # 'MVTec-AD_cable','MVTec-AD_capsule','MVTec-AD_carpet','MVTec-AD_grid','MVTec-AD_hazelnut','MVTec-AD_leather',\n",
    "    # 'MVTec-AD_metal_nut','MVTec-AD_pill','MVTec-AD_screw','MVTec-AD_tile','MVTec-AD_toothbrush',\n",
    "    # 'MVTec-AD_transistor','MVTec-AD_wood','MVTec-AD_zipper'])\n",
    "\n",
    "# Define the mapping of domain names to the corresponding dataset folder paths\n",
    "dataset_links = {\n",
    "    'Classical': 'Data/Classical/',  # Path to Classical domain datasets\n",
    "    'NLP': 'Data/NLP_by_BERT/',      # Path to NLP domain datasets\n",
    "    'CV': 'Data/CV_by_ResNet18/'      # Path to CV domain datasets\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a list to hold the results from dataset processing\n",
    "data_ans = []\n",
    "\n",
    "# Define the path to store the final results (CSV format)\n",
    "output_file = f\"Results/DVMOD_result_realworld_data.csv\"\n",
    "\n",
    "# Define the column headers for the output file, including performance metrics\n",
    "columns = [\n",
    "    \"Dataset\", \"AUCROC\", \"AUCPR\", \"Accuracy\", \"MCC\", \"F1 score\",\n",
    "    \"PPV (Precision)\", \"TPR (Recall)\", \"Time train\", \"Time test\"\n",
    "]\n",
    "\n",
    "__SCALER = 'QuantileTransformer'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file initialization (only write header if file doesn't exist)\n",
    "if not os.path.exists(output_file):\n",
    "    # Create a CSV file with headers if it doesn't exist\n",
    "    pd.DataFrame(columns=columns).to_csv(output_file, index=False)\n",
    "\n",
    "# Function to load and process the dataset (ensure it returns {'X', 'y'})\n",
    "def load_and_process_dataset(name, domain):\n",
    "    \"\"\"\n",
    "    Load and preprocess a dataset for a given domain and scaler.\n",
    "    \n",
    "    Parameters:\n",
    "    - name (str): The name of the dataset (e.g., '1_ALOI', '20news_0').\n",
    "    - domain (str): The domain of the dataset (e.g., 'Classical', 'NLP', 'CV').\n",
    "    \n",
    "    Returns:\n",
    "    - data (dict): A dictionary containing 'X' (features) and 'y' (labels) if loading is successful.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construct the dataset path based on the domain\n",
    "        dataset_path = f\"{dataset_links[domain]}{name}.npz\"\n",
    "        \n",
    "        # Load the dataset from the path using np.load\n",
    "        data = np.load(dataset_path, allow_pickle=True)\n",
    "        \n",
    "        # Return the data in the form of {'X': X_data, 'y': y_data}\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        # In case of an error during loading, print the error and return None\n",
    "        print(f\"Error loading {domain} dataset {name}: {e}\")\n",
    "        return None\n",
    "    \n",
    "def Get_Scaler(name):\n",
    "  # (StandardScaler, MinMaxScaler, RobustScaler, Normalizer)\n",
    "  if name == \"StandardScaler\":\n",
    "    return StandardScaler()\n",
    "  if name == \"MinMaxScaler\":\n",
    "    return MinMaxScaler()\n",
    "  if name == \"RobustScaler\":\n",
    "    return RobustScaler()\n",
    "  if name == \"Normalizer\":\n",
    "    return Normalizer()\n",
    "  if name == \"QuantileTransformer\":\n",
    "      return QuantileTransformer(output_distribution = \"normal\", random_state=42)\n",
    "  if name == \"MaxAbsScaler\":\n",
    "      return MaxAbsScaler()\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through datasets\n",
    "for dataset_array, domain in zip([dataset_name1, dataset_name2, dataset_name3], ['Classical', 'NLP', 'CV']):\n",
    "    for name in dataset_array:\n",
    "        # Load and process the dataset for each dataset name\n",
    "        data = load_and_process_dataset(name, domain)\n",
    "        if data is None:\n",
    "            # Skip the current iteration if data could not be loaded\n",
    "            continue  \n",
    "\n",
    "        try:\n",
    "            # Extract features (X) and labels (y) from the loaded data\n",
    "            X, y = data['X'], data['y']\n",
    "            X, y = pd.DataFrame(X), pd.DataFrame(y)\n",
    "            print(\"Original data size:\", len(y))\n",
    "\n",
    "            # Reduce data size if it's too large (only keep 10,000 samples)\n",
    "            if len(y) > 10000:\n",
    "                print(\"Reducing data size to 10000\")\n",
    "                # Split data and keep only 10000 samples for training and testing\n",
    "                _, X, _, y = train_test_split(X, y, test_size=10000, random_state=42)\n",
    "                print(\"Reduced data size:\", len(y))\n",
    "\n",
    "            # Split data into train/test sets (70% train, 30% test)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "            scaler = Get_Scaler(__SCALER)\n",
    "            scaler.fit(X_train)\n",
    "            X_train = scaler.transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "            X_train, X_test = pd.DataFrame(X_train), pd.DataFrame(X_test)\n",
    "            y_train, y_test = pd.DataFrame(y_train), pd.DataFrame(y_test)\n",
    "\n",
    "\n",
    "            # Combine features and labels for both train and test data\n",
    "            Train_data = pd.concat([X_train, y_train], axis=1, ignore_index=True)\n",
    "            Test_data = pd.concat([X_test, y_test], axis=1, ignore_index=True)\n",
    "\n",
    "            # Track time for model fitting and evaluation\n",
    "            t0 = time.time()\n",
    "            # Preprocess data for Outlier Detection (OC) type\n",
    "            X_train, y_train, X_test, y_test = preprocess_data_OC(Train_data, Test_data)\n",
    "            \n",
    "            # Train the model and make predictions\n",
    "            dvm_od = DVM_OD()\n",
    "            dvm_od.fit(X_train, y_train)                                                         # Fit the model to the training data\n",
    "            t1 = time.time()\n",
    "            y_score = dvm_od.predict(X_test)                                                     # Make predictions on the test data\n",
    "            t2 = time.time()\n",
    "\n",
    "            # Transform the training data using the model's transformation\n",
    "            y_train_score = dvm_od.transform(X_train)\n",
    "\n",
    "            # Calculate distances for evaluation\n",
    "            train_score_tmp = distance_vector(y_train_score, y_train_score)\n",
    "            for i in range(len(train_score_tmp)):\n",
    "                train_score_tmp[i , i] = 1e9                                                      # Set diagonal to a very high value to avoid self-distance\n",
    "            train_score = np.amin(train_score_tmp, axis=1)\n",
    "\n",
    "            # Calculate probabilities and predictions\n",
    "            y_proba = np.zeros((len(y_score), 2))\n",
    "            y_proba[:, 1] = np.minimum(y_score / np.max(train_score), 1)                          # Probability for class 1\n",
    "            y_proba[:, 0] = 1 - y_proba[:, 1]                                                     # Probability for class 0\n",
    "            y_predict = (y_proba[:, 1] > 0.5).astype(int)                                         # Convert probabilities to binary predictions\n",
    "\n",
    "            # Evaluate the model\n",
    "            v = Model_evaluating(y_test, y_predict, y_proba)\n",
    "\n",
    "            # Calculate training and testing times\n",
    "            t_train = t1 - t0\n",
    "            t_test = t2 - t1\n",
    "            result = [name] + v + [t_train, t_test]   \n",
    "            result_df = pd.DataFrame([result], columns=columns)\n",
    "            result_df.to_csv(output_file, mode='a', header=False, index=False)\n",
    "            print(f\"Result appended to {output_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing dataset {name}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
