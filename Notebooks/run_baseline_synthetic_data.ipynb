{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecf694f-e4b9-4d4f-aa4b-54a8327d98fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    matthews_corrcoef, f1_score, precision_score, recall_score,\n",
    "    accuracy_score, roc_auc_score, average_precision_score\n",
    ")\n",
    "from sklearn.preprocessing import KernelCenterer,LabelEncoder, MinMaxScaler, Normalizer, QuantileTransformer, RobustScaler\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.deep_svdd import DeepSVDD\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.pca import PCA\n",
    "from pyod.models.sod import SOD\n",
    "from pyod.models.cof import COF\n",
    "from pyod.models.loda import LODA\n",
    "from pyod.models.ecod import ECOD\n",
    "from pyod.models.copod import COPOD\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.models.alad import ALAD\n",
    "from pyod.models.ae1svm import AE1SVM\n",
    "from pyod.models.devnet import DevNet\n",
    "from pyod.models.lunar import LUNAR\n",
    "\n",
    "def load_and_process_dataset_X(dataset_links, dataset_name):\n",
    "    \"\"\"\n",
    "    Loads the features (X) dataset from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_links (str): The directory path where the dataset files are stored.\n",
    "    - dataset_name (str): The name of the dataset (excluding the suffix).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The features (X) dataset as a Pandas DataFrame, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construct the path based on the domain and dataset name\n",
    "        path = os.path.join(dataset_links, f\"{dataset_name}_X.csv\")\n",
    "        \n",
    "        # Load the dataset (assuming CSV format)\n",
    "        data = pd.read_csv(path)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading X dataset {dataset_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to load and process y dataset from CSV\n",
    "def load_and_process_dataset_y(dataset_links, dataset_name):\n",
    "    \"\"\"\n",
    "    Loads the labels (y) dataset from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_links (str): The directory path where the dataset files are stored.\n",
    "    - dataset_name (str): The name of the dataset (excluding the suffix).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The labels (y) dataset as a Pandas DataFrame, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construct the path based on the domain and dataset name\n",
    "        path = os.path.join(dataset_links, f\"{dataset_name}_y.csv\")\n",
    "        \n",
    "        # Load the dataset (assuming CSV format)\n",
    "        data = pd.read_csv(path)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading y dataset {dataset_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to preprocess the data (split into features and labels)\n",
    "def preprocess_data(train_data, test_data):\n",
    "    \"\"\"\n",
    "    Preprocesses the training and test data by separating features (X) and labels (y).\n",
    "    \n",
    "    Parameters:\n",
    "    - train_data (pd.DataFrame): The training data including both features and labels.\n",
    "    - test_data (pd.DataFrame): The test data including both features and labels.\n",
    "\n",
    "    Returns:\n",
    "    - X_train (np.ndarray): The feature set for training data (excluding the labels).\n",
    "    - y_train (np.ndarray): The label set for training data (the last column).\n",
    "    - X_test (np.ndarray): The feature set for test data (excluding the labels).\n",
    "    - y_test (np.ndarray): The label set for test data (the last column).\n",
    "    \"\"\"\n",
    "    print(\"..............................Data Overview................................\")\n",
    "    print(\"Train Data Shape:\", train_data.shape)\n",
    "    print(\"Test Data Shape:\", test_data.shape)\n",
    "\n",
    "    # Convert to numpy for easier manipulation\n",
    "    X_train_total = train_data.iloc[:, :-1].to_numpy()  # Extract features (all columns except last)\n",
    "    y_train_total = train_data.iloc[:, -1].to_numpy()   # Extract labels (last column)\n",
    "\n",
    "    # Filter training data for the class label 0 (normal class)\n",
    "    X_train = X_train_total[y_train_total == 0]\n",
    "    y_train = y_train_total[y_train_total == 0]\n",
    "\n",
    "    print(\"Train Data Labels [0]:\", np.unique(y_train))  # Print the unique labels for training data\n",
    "\n",
    "    # Prepare test data\n",
    "    X_test = test_data.iloc[:, :-1].to_numpy()  # Extract features from test data\n",
    "    y_test = test_data.iloc[:, -1].to_numpy()   # Extract labels from test data\n",
    "\n",
    "    # Display the number of samples and features in training data\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_features = X_train.shape[1]\n",
    "    print(\"Number of samples:\", n_samples)\n",
    "    print(\"Number of features:\", n_features)\n",
    "\n",
    "    # Return the preprocessed data\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Function to evaluate the model using various metrics\n",
    "def evaluate_model(y_true, y_pred, y_scores=None, y_probabilities=None):\n",
    "    \"\"\"\n",
    "    Evaluates the model using multiple metrics and prints the results.\n",
    "    This version includes AUCROC and AUCPR using predicted probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true (np.ndarray): True labels of the test data.\n",
    "    - y_pred (np.ndarray): Predicted labels of the test data.\n",
    "    - y_scores (np.ndarray, optional): Scores used to compute AUCROC and AUCPR.\n",
    "    - y_probabilities (np.ndarray, optional): Predicted probabilities for each class.\n",
    "\n",
    "    Returns:\n",
    "    - metrics (list): A list containing AUCROC, AUCPR, accuracy, MCC, F1 score, precision, and recall.\n",
    "    \"\"\"\n",
    "    print(\"..............................Evaluation Metrics...............................\")\n",
    "\n",
    "    # Calculate standard metrics\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)  # Matthew's correlation coefficient\n",
    "    f1 = f1_score(y_true, y_pred)  # F1 score\n",
    "    precision = precision_score(y_true, y_pred)  # Precision\n",
    "    recall = recall_score(y_true, y_pred)  # Recall\n",
    "    accuracy = accuracy_score(y_true, y_pred)  # Accuracy\n",
    "\n",
    "    # ROC and PR curve scores using predicted probabilities\n",
    "    auc_roc, auc_pr = None, None\n",
    "    if y_probabilities is not None:\n",
    "        auc_roc = roc_auc_score(y_true, y_probabilities[:, 1])  # Probabilities for class 1 (outliers)\n",
    "        auc_pr = average_precision_score(y_true, y_probabilities[:, 1])  # AUCPR for class 1\n",
    "\n",
    "    # Display metrics\n",
    "    print(f\"AUCROC: {auc_roc * 100 if auc_roc else 'N/A'}\")\n",
    "    print(f\"AUCPR: {auc_pr * 100 if auc_pr else 'N/A'}\")\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}\")\n",
    "    print(f\"MCC: {mcc:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "    # Return the evaluation metrics as a list\n",
    "    return [auc_roc * 100 if auc_roc else None, auc_pr * 100 if auc_pr else None,\n",
    "            accuracy * 100, mcc, f1, precision, recall]\n",
    "\n",
    "# Function to return the appropriate model based on the name\n",
    "def get_model(name, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns the appropriate PyOD model based on the provided name.\n",
    "    \n",
    "    Parameters:\n",
    "    - name (str): The name of the anomaly detection model (e.g., 'CBLOF', 'KNN').\n",
    "    - kwargs (dict): Additional parameters to initialize the model.\n",
    "\n",
    "    Returns:\n",
    "    - model (object): An instance of the specified anomaly detection model.\n",
    "    \"\"\"\n",
    "    model_dict = {\n",
    "        \"CBLOF\": CBLOF,\n",
    "        \"KNN\": KNN,\n",
    "        \"IForest\": IForest,\n",
    "        \"OCSVM\": OCSVM,\n",
    "        \"LOF\": LOF,\n",
    "        \"DeepSVDD\": DeepSVDD,\n",
    "        \"HBOS\": HBOS,\n",
    "        \"PCA\": PCA,\n",
    "        \"SOD\": SOD,\n",
    "        \"COF\": COF,\n",
    "        \"LODA\": LODA,\n",
    "        \"ECOD\": ECOD,\n",
    "        \"COPOD\": COPOD,\n",
    "        \"AutoEncoder\": AutoEncoder,\n",
    "        \"DevNet\": DevNet,\n",
    "        \"LUNAR\": LUNAR,\n",
    "        \"AE1SVM\": AE1SVM,\n",
    "        \"ALAD\": ALAD\n",
    "    }\n",
    "    \n",
    "    # Fetch the model class based on the provided model name\n",
    "    model_class = model_dict.get(name)\n",
    "    \n",
    "    # Raise an error if the model name is not found\n",
    "    if model_class is None:\n",
    "        raise ValueError(f\"Model {name} not found.\")\n",
    "    \n",
    "    # Return the model initialized with the provided parameters\n",
    "    return model_class(**kwargs)\n",
    "\n",
    "\n",
    "dataset_names = np.array(['1_ALOI', '2_annthyroid', '3_backdoor', '4_breastw', '5_campaign', '6_cardio', '7_Cardiotocography', '8_celeba', \n",
    "                         '9_census', '10_cover', '11_donors', '12_fault', '13_fraud', '14_glass', '15_Hepatitis', '16_http', '17_InternetAds',\n",
    "                         '18_Ionosphere', '19_landsat', '20_letter', '21_Lymphography', '22_magic.gamma', '23_mammography', '24_mnist', '25_musk',\n",
    "                         '26_optdigits', '27_PageBlocks', '28_pendigits', '29_Pima', '30_satellite', '31_satimage-2', '32_shuttle', '33_skin', \n",
    "                         '34_smtp', '35_SpamBase', '36_speech', '37_Stamps', '38_thyroid', '39_vertebral', '40_vowels', '41_Waveform', '42_WBC',\n",
    "                         '43_WDBC', '44_Wilt','20news_0', '20news_1', '20news_2', '20news_3', '20news_4', '20news_5', 'agnews_0', 'agnews_1', 'agnews_2',\n",
    "                         'agnews_3', 'amazon', 'imdb', 'yelp', '45_wine', '46_WPBC', '47_yeast','CIFAR10_0', 'CIFAR10_1', 'CIFAR10_2', 'CIFAR10_3', 'CIFAR10_4',\n",
    "                         'CIFAR10_5','CIFAR10_6','CIFAR10_7', 'CIFAR10_8','CIFAR10_9','FashionMNIST_0','FashionMNIST_1','FashionMNIST_2','FashionMNIST_3','FashionMNIST_4','FashionMNIST_5',\n",
    "                         'FashionMNIST_6','FashionMNIST_7','FashionMNIST_8','FashionMNIST_9','SVHN_0','SVHN_1','SVHN_2','SVHN_3','SVHN_4',\n",
    "                         'SVHN_5','SVHN_6','SVHN_7','SVHN_8','SVHN_9','MNIST-C_brightness', 'MNIST-C_canny_edges','MNIST-C_dotted_line', 'MNIST-C_fog',\n",
    "                         'MNIST-C_glass_blur','MNIST-C_identity','MNIST-C_impulse_noise','MNIST-C_motion_blur','MNIST-C_rotate','MNIST-C_scale','MNIST-C_shear',\n",
    "                         'MNIST-C_shot_noise','MNIST-C_spatter','MNIST-C_stripe','MNIST-C_translate','MNIST-C_zigzag','MVTec-AD_bottle',\n",
    "                         'MVTec-AD_cable','MVTec-AD_capsule','MVTec-AD_carpet','MVTec-AD_grid','MVTec-AD_hazelnut','MVTec-AD_leather',\n",
    "                         'MVTec-AD_metal_nut','MVTec-AD_pill','MVTec-AD_screw','MVTec-AD_tile','MVTec-AD_toothbrush',\n",
    "                         'MVTec-AD_transistor','MVTec-AD_wood','MVTec-AD_zipper'])\n",
    "models_list = [\n",
    "    \"CBLOF\",\n",
    "    \"KNN\",\n",
    "    \"IForest\",\n",
    "    \"OCSVM\",\n",
    "    \"LOF\",\n",
    "    \"DeepSVDD\",\n",
    "    \"HBOS\",\n",
    "    \"PCA\",\n",
    "    \"SOD\",\n",
    "    \"COF\",\n",
    "    \"LODA\",\n",
    "    \"ECOD\",\n",
    "    \"COPOD\",\n",
    "    \"AutoEncoder\"\n",
    "     \"DevNet\",\n",
    "    \"LUNAR\",\n",
    "    \"AE1SVM\",\n",
    "    \"ALAD\"\n",
    "]\n",
    "\n",
    "output_file = f\"./HHHD Model/Raw_Data_Baseline/Raw_Baseline_OC_lc.csv\"\n",
    "columns = [\"Dataset\", \"Model\", \"AUCROC\", \"AUCPR\", \"Accuracy\", \"MCC\", \"F1 Score\",\n",
    "           \"Precision\", \"Recall\", \"Time Train\", \"Time Test\"]\n",
    "\n",
    "# Initialize output CSV file\n",
    "if not os.path.exists(output_file):\n",
    "    pd.DataFrame(columns=columns).to_csv(output_file, index=False)\n",
    "\n",
    "# Define a function to run the model training and evaluation\n",
    "def run_experiment(dataset_names):\n",
    "    for name in dataset_names:\n",
    "        for model_name in models_list:\n",
    "            try:\n",
    "                dataset_links = 'Synthetic_Datasets/local_outliers_datasets/'\n",
    "                print(f\"\\nRunning dataset {name} with model {model_name}\")\n",
    "\n",
    "                X = load_and_process_dataset_X(dataset_links, name)\n",
    "                y = load_and_process_dataset_y(dataset_links, name)\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "                train_data = pd.concat([X_train, y_train], axis=1, ignore_index=True)\n",
    "                test_data = pd.concat([X_test, y_test], axis=1, ignore_index=True)\n",
    "\n",
    "                X_train, y_train, X_test, y_test = preprocess_data(train_data, test_data)\n",
    "\n",
    "                if model_name == 'DeepSVDD':\n",
    "                    start_time = time.time()\n",
    "                    n_features = X_train.shape[1]\n",
    "                    model = DeepSVDD(n_features=n_features)\n",
    "                    model.fit(X_train)\n",
    "                else:\n",
    "                    model = get_model(model_name)\n",
    "                    start_time = time.time()\n",
    "                    if model_name == 'DevNet':\n",
    "                        model.fit(X_train, y_train)\n",
    "                    else:\n",
    "                        model.fit(X_train)\n",
    "                        \n",
    "                train_time = time.time() - start_time\n",
    "\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                if hasattr(model, \"predict_proba\"):\n",
    "                    y_probabilities = model.predict_proba(X_test)\n",
    "                else:\n",
    "                    y_probabilities = None\n",
    "\n",
    "                test_time = time.time() - start_time\n",
    "\n",
    "                metrics = evaluate_model(y_test, y_pred, y_scores=None, y_probabilities=y_probabilities)\n",
    "                result = [name, model_name] + metrics + [train_time, test_time]\n",
    "                result_df = pd.DataFrame([result], columns=columns)\n",
    "                result_df.to_csv(output_file, mode='a', header=False, index=False)\n",
    "\n",
    "                print(f\"Results saved for {name} with model {model_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with dataset {name}, model {model_name}: {e}\")\n",
    "\n",
    "run_experiment(dataset_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
