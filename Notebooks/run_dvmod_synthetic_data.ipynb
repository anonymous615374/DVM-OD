{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os \n",
    "\n",
    "from Model import DVM_OD\n",
    "\n",
    "from sklearn.metrics import (roc_auc_score, precision_score, average_precision_score, recall_score,f1_score, accuracy_score,matthews_corrcoef)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, QuantileTransformer, RobustScaler, MinMaxScaler, MaxAbsScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_vector(null_point_X, null_point_Y):\n",
    "    norm_X = np.sum(null_point_X**2, axis=1)  # (N_train,)\n",
    "    norm_Y = np.sum(null_point_Y**2, axis=1)  # (N_test,)\n",
    "    dot_product = np.dot(null_point_Y, null_point_X.T)  # (N_test, N_train)\n",
    "    distance = np.sqrt(abs(norm_Y[:, np.newaxis] + norm_X[np.newaxis, :] - 2 * dot_product))\n",
    "    return distance\n",
    "    \n",
    "def preprocess_data_OC(train_data, test_data):\n",
    "    \"\"\"\n",
    "    Preprocess the train and test data by separating features and labels,\n",
    "    and return the preprocessed feature sets and labels.\n",
    "    \"\"\"\n",
    "    print(\"..............................Data Overview................................\")\n",
    "    print(\"Train Data Shape:\", train_data.shape)\n",
    "    print(\"Test Data Shape:\", test_data.shape)\n",
    "\n",
    "    # Convert to numpy for easier manipulation\n",
    "    X_train_total = train_data.iloc[:, :-1].to_numpy()\n",
    "    y_train_total = train_data.iloc[:, -1].to_numpy()\n",
    "\n",
    "    X_train = X_train_total[y_train_total == 0]\n",
    "    y_train = y_train_total[y_train_total == 0]\n",
    "\n",
    "    print(\"Train Data Labels [0]:\", np.unique(y_train))\n",
    "\n",
    "    # Prepare test data\n",
    "    X_test = test_data.iloc[:, :-1].to_numpy()\n",
    "    y_test = test_data.iloc[:, -1].to_numpy()\n",
    "\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_features = X_train.shape[1]\n",
    "    print(\"Number of samples:\", n_samples)\n",
    "    print(\"Number of features:\", n_features)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "  \n",
    "\n",
    "def Model_evaluating(y_true, y_predict, y_scores ):\n",
    "    print(\"..............................Report Parameter...............................\")\n",
    "    \n",
    "    mcc = matthews_corrcoef(y_true, y_predict)\n",
    "    \n",
    "    f1 = f1_score(y_true, y_predict)\n",
    "    \n",
    "    ppv = precision_score(y_true, y_predict)\n",
    "    \n",
    "    tpr = recall_score(y_true, y_predict)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_predict)\n",
    "    \n",
    "    AUC = roc_auc_score(y_true, y_scores[:,1])\n",
    "    \n",
    "    AUCPR = average_precision_score(y_true, y_scores[:,1])\n",
    "    \n",
    "    # Print out the evaluation metrics\n",
    "    print(\"AUCROC:\", AUC * 100)\n",
    "    print(\"AUCPR:\", AUCPR * 100)\n",
    "    print(\"Accuracy:\", accuracy * 100)\n",
    "    print(\"MCC:\", mcc)\n",
    "    print(\"F1 score:\", f1)\n",
    "    print(\"PPV (Precision):\", ppv)\n",
    "    print(\"TPR (Recall):\", tpr)\n",
    "    return [AUC *100, AUCPR * 100, accuracy * 100, mcc, f1, ppv, tpr]\n",
    "    \n",
    "\n",
    "def Get_Scaler(name):\n",
    "  # (StandardScaler, MinMaxScaler, RobustScaler, Normalizer)\n",
    "  if name == \"StandardScaler\":\n",
    "    return StandardScaler()\n",
    "  if name == \"MinMaxScaler\":\n",
    "    return MinMaxScaler()\n",
    "  if name == \"RobustScaler\":\n",
    "    return RobustScaler()\n",
    "  if name == \"Normalizer\":\n",
    "    return Normalizer()\n",
    "  if name == \"QuantileTransformer\":\n",
    "      return QuantileTransformer(output_distribution = \"normal\", random_state=42)\n",
    "  if name == \"MaxAbsScaler\":\n",
    "      return MaxAbsScaler()\n",
    "  return None\n",
    "\n",
    "\n",
    "def get_top_x_percent(y_score, x):\n",
    "    \"\"\"\n",
    "    The function gets the element in the top x% of the y_score array.\n",
    "\n",
    "    Parameters:\n",
    "    y_score (array-like): The input data array (can be a list or a numpy array).\n",
    "    x (float): The percentage (for example, x = 5 for top 5%).\n",
    "\n",
    "    Returns:\n",
    "    float: The value in the top x% of the y_score array.\n",
    "    \"\"\"\n",
    "    sorted_scores = np.sort(y_score)\n",
    "    \n",
    "    \n",
    "    top_index = int(len(sorted_scores) * ( x / 100))\n",
    "    \n",
    "    \n",
    "    return sorted_scores[top_index]   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the three dataset name arrays\n",
    "## You can uncomment the datasets you want to run the model on\n",
    "dataset_names = np.array(['1_ALOI'])\n",
    "                        #   , '2_annthyroid', '3_backdoor', '4_breastw', '5_campaign', '6_cardio', '7_Cardiotocography', '8_celeba', \n",
    "                        #  '9_census', '10_cover', '11_donors', '12_fault', '13_fraud', '14_glass', '15_Hepatitis', '16_http', '17_InternetAds',\n",
    "                        #  '18_Ionosphere', '19_landsat', '20_letter', '21_Lymphography', '22_magic.gamma', '23_mammography', '24_mnist', '25_musk',\n",
    "                        #  '26_optdigits', '27_PageBlocks', '28_pendigits', '29_Pima', '30_satellite', '31_satimage-2', '32_shuttle', '33_skin', \n",
    "                        #  '34_smtp', '35_SpamBase', '36_speech', '37_Stamps', '38_thyroid', '39_vertebral', '40_vowels', '41_Waveform', '42_WBC',\n",
    "                        #  '43_WDBC', '44_Wilt', '45_wine', '46_WPBC', '47_yeast'])\n",
    "\n",
    "## You can change the type of the dataset to run the model on\n",
    "__TYPE = \"global\"\n",
    "__SCALER = \"QuantileTransformer\"\n",
    "\n",
    "data_ans = []\n",
    "\n",
    "output_file = f\"Results/DVMOD_result_{__TYPE}_data.csv\"\n",
    "\n",
    "columns = [\n",
    "    \"Dataset\", \"AUCROC\", \"AUCPR\", \"Accuracy\", \"MCC\", \"F1 score\",\n",
    "    \"PPV (Precision)\", \"TPR (Recall)\", \"Time train\", \"Time test\", \"Threshold\"]\n",
    "\n",
    "\n",
    "# Define the dataset links (paths to the data)\n",
    "dataset_links = f'Data/Synthetic_Datasets/{__TYPE}_outliers_datasets/'\n",
    "\n",
    "# Function to load and process X dataset from CSV\n",
    "def load_and_process_dataset_X(dataset_name):\n",
    "    try:\n",
    "        # Construct the path based on the domain and dataset name\n",
    "        path = f\"{dataset_links}{dataset_name}_X.csv\"\n",
    "        \n",
    "        # Load the dataset (assuming CSV format)\n",
    "        data = pd.read_csv(path)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading X dataset {dataset_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to load and process y dataset from CSV\n",
    "def load_and_process_dataset_y(dataset_name):\n",
    "    try:\n",
    "        # Construct the path based on the domain and dataset name\n",
    "        path = f\"{dataset_links}{dataset_name}_y.csv\"\n",
    "        \n",
    "        # Load the dataset (assuming CSV format)\n",
    "        data = pd.read_csv(path)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading y dataset {dataset_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Output file initialization (only write header if file doesn't exist)\n",
    "if not os.path.exists(output_file):\n",
    "    pd.DataFrame(columns=columns).to_csv(output_file, index=False)\n",
    "\n",
    "# Iterate through datasets\n",
    "for dataset_name in dataset_names:\n",
    "    X = load_and_process_dataset_X(dataset_name)\n",
    "    y = load_and_process_dataset_y(dataset_name)\n",
    "\n",
    "    if X is None or y is None:\n",
    "        continue  # Skip if the dataset couldn't be loaded\n",
    "\n",
    "    try:\n",
    "        print(\"Original data size:\", len(y))\n",
    "\n",
    "        # Split data into train/test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        Train_data = pd.concat([X_train, y_train], axis=1, ignore_index=True)\n",
    "        Test_data = pd.concat([X_test, y_test], axis=1, ignore_index=True)\n",
    "\n",
    "        # Track time for model fitting and evaluation\n",
    "        t0 = time.time()\n",
    "        \n",
    "        \n",
    "        # Apply preprocessing (assuming preprocess_data_OC function exists)\n",
    "        X_train, y_train, X_test, y_test = preprocess_data_OC(Train_data, Test_data)\n",
    "\n",
    "        scaler = Get_Scaler(__SCALER)\n",
    "        scaler.fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Train the model and make predictions\n",
    "        dvm_od = DVM_OD()\n",
    "        dvm_od.fit(X_train, y_train)                                                         # Fit the model to the training data\n",
    "        t1 = time.time()\n",
    "        y_score = dvm_od.predict(X_test)                                                     # Make predictions on the test data\n",
    "        t2 = time.time()\n",
    "\n",
    "        y_train_score = dvm_od.transform(X_train)\n",
    "        train_score_tmp = distance_vector(y_train_score, y_train_score)\n",
    "        for i in range(len(train_score_tmp)):\n",
    "                train_score_tmp[i , i] = 1e9\n",
    "        train_score = np.amin(train_score_tmp, axis=1)\n",
    "\n",
    "        nu = get_top_x_percent(train_score, 99)\n",
    "        \n",
    "        \n",
    "        # Calculate probabilities and predictions\n",
    "        y_proba = np.zeros((len(y_score), 2))\n",
    "        y_proba[:, 1] = np.minimum(y_score / nu, 1)  # Probability for class 1\n",
    "        y_proba[:, 0] = 1 - y_proba[:, 1]           # Probability for class 0\n",
    "        y_predict = (y_proba[:, 1] > 0.).astype(int)\n",
    "\n",
    "        # Model evaluation (assuming Model_evaluating function exists)\n",
    "        v = Model_evaluating(y_test, y_predict, y_proba)\n",
    "\n",
    "        t_train = t1 - t0\n",
    "        t_test = t2 - t1\n",
    "\n",
    "        # Prepare result for appending\n",
    "        result = [dataset_name] + v + [t_train, t_test] + [nu]\n",
    "        result_df = pd.DataFrame([result], columns=columns)\n",
    "        result_df.to_csv(output_file, mode='a', header=False, index=False)\n",
    "        print(f\"Result appended to {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset {dataset_name}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
